# Project Overview

This repository implements a multi-modal emotion recognition system
using audio and text data from the IEMOCAP dataset. The model leverages
a combination of deep learning techniques, including MSCNN, statistical
pooling unit (SPU), and attention mechanisms, along with GloVe
embeddings for text representation.

# Dependencies

To install the necessary dependencies, run:

    pip install -r requirements.txt

# File Structure

-   **requirements.txt**: List of dependencies.

-   **train.py**: Training script for the model.

-   **test.py**: Testing and evaluation script.

-   **analysis.py**: code for plots, classification report and confusion
    matrix

-   **README.md**: readme file, Project details.

-   **EE798_implementation_PDF.pdf**: Project results, and dataset description.

# How to Run

-   **Training the model:**

            train.py

-   **Testing:**

            test.py

-   **Evaluation:**

            Evaluation.py

# Results

After running the training and testing scripts, you can visualize the
training history (accuracy plot) and confusion matrix.
